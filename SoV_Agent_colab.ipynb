{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab cell: install dependencies (run once)\n",
    "!pip install --quiet cohere python-dotenv google-api-python-client pandas matplotlib requests tenacity\n",
    "\n",
    "# ----------------------------\n",
    "# Colab-ready SOV agent script\n",
    "# ----------------------------\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import cohere\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create the sentiment analysis pipeline just once at the top\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    truncation = True,\n",
    "    max_length = 512\n",
    ")\n",
    "\n",
    "\n",
    "# Load .env (make sure .env contains COHERE_API_KEY, YOUTUBE_API_KEY, HF_API_KEY)\n",
    "#load_dotenv()\n",
    "COHERE_API_KEY = 'mc4FkwQcGSV1QyJonOq1rEBOPZzeovm6uNfeBWHl'\n",
    "YOUTUBE_API_KEY = 'AIzaSyBx3CCgsqrwsVBEMeoQ4fcZA1jzue12SQ4'\n",
    "HF_API_KEY = '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'\n",
    "\n",
    "if not (COHERE_API_KEY and YOUTUBE_API_KEY and HF_API_KEY):\n",
    "    raise ValueError(\"Set COHERE_API_KEY, YOUTUBE_API_KEY and HF_API_KEY in your .env file\")\n",
    "\n",
    "# Clients / config\n",
    "co = cohere.Client(COHERE_API_KEY)\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=YOUTUBE_API_KEY)\n",
    "HF_HEADERS = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
    "HF_BASE = \"https://api-inference.huggingface.co/models/\"\n",
    "#SENTIMENT_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "MAX_KEYWORDS = 10\n",
    "VIDEOS_PER_KEYWORD = 5\n",
    "COMMENTS_PER_VIDEO = 30\n",
    "\n",
    "# Hard-coded competitor list (Atomberg + rivals)\n",
    "COMPETITORS = [\"Atomberg\", \"Crompton\", \"Havells\", \"Orient Electric\", \"Usha\", \"Bajaj\"]\n",
    "\n",
    "# ---------- HF inference wrapper with retry ----------\n",
    "@retry(wait=wait_exponential(multiplier=1, min=1, max=12), stop=stop_after_attempt(5),\n",
    "       retry=retry_if_exception_type(requests.exceptions.RequestException))\n",
    "def hf_inference(model_name, payload, timeout=30):\n",
    "    url = HF_BASE + model_name\n",
    "    resp = requests.post(url, headers=HF_HEADERS, json=payload, timeout=timeout)\n",
    "    if resp.status_code == 429:\n",
    "        # raise to trigger tenacity retry/backoff\n",
    "        raise requests.exceptions.RequestException(\"429 rate limit from HF\")\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def sentiment_of_text(text):\n",
    "    \"\"\"Return ('positive'|'neutral'|'negative', score) using HF pipeline.\"\"\"\n",
    "    try:\n",
    "        # pipeline automatically tokenizes and truncates if needed\n",
    "        out = sentiment_pipeline(text[:5000])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Sentiment analysis failed: {e}\")\n",
    "        return \"neutral\", 0.0\n",
    "\n",
    "    if isinstance(out, list) and out:\n",
    "        label = out[0].get(\"label\", \"\")\n",
    "        score = float(out[0].get(\"score\", 0.0))\n",
    "        mapping = {\n",
    "            \"LABEL_0\": \"negative\", \"LABEL_1\": \"neutral\", \"LABEL_2\": \"positive\",\n",
    "            \"NEGATIVE\": \"negative\", \"NEUTRAL\": \"neutral\", \"POSITIVE\": \"positive\"\n",
    "        }\n",
    "        return mapping.get(label, label.lower()), score\n",
    "\n",
    "    return \"neutral\", 0.0\n",
    "\n",
    "# ---------- Cohere keyword expansion ----------\n",
    "def generate_keywords_cohere(base_keyword, max_k=MAX_KEYWORDS):\n",
    "    prompt = (\n",
    "        f\"Generate up to {max_k-1} related search keywords for: '{base_keyword}'. \"\n",
    "        \"Avoid brand names. Return a Python list like ['kw1','kw2', ...].\"\n",
    "    )\n",
    "    try:\n",
    "        resp = co.generate(model=\"command-xlarge\", prompt=prompt, max_tokens=120, temperature=0.5)\n",
    "        raw = resp.generations[0].text.strip()\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Cohere keyword expansion failed:\", e)\n",
    "        return [base_keyword]\n",
    "\n",
    "    # try to parse Python literal list safely\n",
    "    try:\n",
    "        keywords = eval(raw, {\"__builtins__\":{}})\n",
    "        if isinstance(keywords, list):\n",
    "            kws = [base_keyword] + [k.strip() for k in keywords[:max_k-1]]\n",
    "            # dedupe preserving order\n",
    "            seen = set()\n",
    "            out = []\n",
    "            for k in kws:\n",
    "                if k.lower() not in seen:\n",
    "                    out.append(k)\n",
    "                    seen.add(k.lower())\n",
    "            return out[:max_k]\n",
    "    except Exception:\n",
    "        # fallback: split by newlines/commas\n",
    "        parts = re.split(r'[\\n,]+', raw)\n",
    "        parts = [p.strip(\" []'\\\"\") for p in parts if p.strip()]\n",
    "        kws = [base_keyword] + parts\n",
    "        seen = set(); out=[]\n",
    "        for k in kws:\n",
    "            if k.lower() not in seen:\n",
    "                out.append(k); seen.add(k.lower())\n",
    "        return out[:max_k]\n",
    "\n",
    "# ---------- YouTube helpers ----------\n",
    "def search_youtube_videos(query, max_results=5000):\n",
    "    resp = youtube.search().list(q=query, part=\"snippet\", type=\"video\", maxResults=max_results).execute()\n",
    "    ids = [item[\"id\"][\"videoId\"] for item in resp.get(\"items\", [])]\n",
    "    return ids\n",
    "\n",
    "def get_video_details(video_ids):\n",
    "    results = []\n",
    "    if not video_ids:\n",
    "        return results\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch = video_ids[i:i+50]\n",
    "        resp = youtube.videos().list(part=\"snippet,statistics\", id=\",\".join(batch)).execute()\n",
    "        for it in resp.get(\"items\", []):\n",
    "            snip = it.get(\"snippet\", {})\n",
    "            stats = it.get(\"statistics\", {})\n",
    "            results.append({\n",
    "                \"video_id\": it[\"id\"],\n",
    "                \"title\": snip.get(\"title\", \"\"),\n",
    "                \"description\": snip.get(\"description\", \"\"),\n",
    "                \"channel\": snip.get(\"channelTitle\", \"\"),\n",
    "                \"view_count\": int(stats.get(\"viewCount\", 0)),\n",
    "            })\n",
    "    return results\n",
    "\n",
    "def get_video_comments(video_id, max_comments=COMMENTS_PER_VIDEO):\n",
    "    comments = []\n",
    "    try:\n",
    "        resp = youtube.commentThreads().list(videoId=video_id, part=\"snippet\", maxResults=min(100, max_comments), textFormat=\"plainText\").execute()\n",
    "        for it in resp.get(\"items\", []):\n",
    "            comments.append(it[\"snippet\"][\"topLevelComment\"][\"snippet\"].get(\"textDisplay\", \"\"))\n",
    "    except Exception:\n",
    "        # comments can be disabled for some videos\n",
    "        pass\n",
    "    return comments\n",
    "\n",
    "# ---------- NEW: explicit competitor mention analysis ----------\n",
    "def analyze_video_mentions(video, competitors, do_sentiment=True):\n",
    "    \"\"\"\n",
    "    video: dict with title, description, comments list, view_count\n",
    "    competitors: list of brand strings\n",
    "    returns: dict per brand: {'mentions': n, 'positive_mentions': m, 'views_attributed': views_if_mentioned}\n",
    "    \"\"\"\n",
    "    title = video.get(\"title\",\"\")\n",
    "    desc = video.get(\"description\",\"\")\n",
    "    comments = video.get(\"comments\", [])\n",
    "    views = video.get(\"view_count\", 0)\n",
    "\n",
    "    text_blob = f\"{title} {desc}\".lower()\n",
    "    brand_data = {b: {\"mentions\":0, \"positive_mentions\":0, \"views_attributed\":0} for b in competitors}\n",
    "\n",
    "    # Title/description mentions: count occurrences\n",
    "    for b in competitors:\n",
    "        # use word boundaries for safety\n",
    "        pattern = r\"\\b\" + re.escape(b.lower()) + r\"\\b\"\n",
    "        count_td = len(re.findall(pattern, text_blob, flags=re.IGNORECASE))\n",
    "        if count_td > 0:\n",
    "            brand_data[b][\"mentions\"] += count_td\n",
    "            brand_data[b][\"views_attributed\"] += views\n",
    "\n",
    "    # Per-comment: check mention, call sentiment (only if comment mentions the brand)\n",
    "    for c in comments:\n",
    "        c_text = c.strip()\n",
    "        if len(c_text) < 3:\n",
    "            continue\n",
    "        lower_c = c_text.lower()\n",
    "        for b in competitors:\n",
    "            if re.search(r\"\\b\" + re.escape(b.lower()) + r\"\\b\", lower_c, flags=re.IGNORECASE):\n",
    "                # increment mention\n",
    "                brand_data[b][\"mentions\"] += 1\n",
    "                # optional: run sentiment for this comment and increment positive_mentions if positive\n",
    "                if do_sentiment:\n",
    "                    label, score = sentiment_of_text(c_text)\n",
    "                    # throttle HF calls lightly\n",
    "                    time.sleep(0.25 + random.random()*0.15)\n",
    "                    if label == \"positive\":\n",
    "                        brand_data[b][\"positive_mentions\"] += 1\n",
    "    return brand_data\n",
    "\n",
    "# ---------- SOV calculation (uses analyze_video_mentions explicitly) ----------\n",
    "def calculate_sov(keywords, competitors=COMPETITORS, videos_per_kw=VIDEOS_PER_KEYWORD):\n",
    "    # initialize aggregated stats\n",
    "    agg = {b: {\"mentions\":0, \"positive_mentions\":0, \"views\":0, \"videos_mentioned_in\":0} for b in competitors}\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(f\"[INFO] fetching videos for '{kw}'\")\n",
    "        ids = search_youtube_videos(kw, max_results=videos_per_kw)\n",
    "        details = get_video_details(ids)\n",
    "        # get comments for each video and attach to details\n",
    "        for d in details:\n",
    "            d[\"comments\"] = get_video_comments(d[\"video_id\"], max_comments=COMMENTS_PER_VIDEO)\n",
    "\n",
    "            # analyze mentions for this video (this is where we explicitly check the competitor list)\n",
    "            per_brand = analyze_video_mentions(d, competitors, do_sentiment=True)\n",
    "            for b, stats in per_brand.items():\n",
    "                if stats[\"mentions\"] > 0:\n",
    "                    agg[b][\"mentions\"] += stats[\"mentions\"]\n",
    "                    agg[b][\"positive_mentions\"] += stats[\"positive_mentions\"]\n",
    "                    # views_attributed added above for td mentions\n",
    "                    agg[b][\"views\"] += stats[\"views_attributed\"]\n",
    "                    agg[b][\"videos_mentioned_in\"] += (1 if stats[\"views_attributed\"]>0 or stats[\"mentions\"]>0 else 0)\n",
    "\n",
    "    # Build DataFrame\n",
    "    total_mentions = sum(agg[b][\"mentions\"] for b in competitors) or 1\n",
    "    total_pos = sum(agg[b][\"positive_mentions\"] for b in competitors) or 1\n",
    "    rows = []\n",
    "    for b in competitors:\n",
    "        m = agg[b][\"mentions\"]\n",
    "        pm = agg[b][\"positive_mentions\"]\n",
    "        v = agg[b][\"views\"]\n",
    "        sov = (m / total_mentions) * 100\n",
    "        pos_sov = (pm / total_pos) * 100 if total_pos>0 else 0.0\n",
    "        rows.append({\n",
    "            \"brand\": b,\n",
    "            \"mentions\": m,\n",
    "            \"positive_mentions\": pm,\n",
    "            \"views_attributed\": v,\n",
    "            \"SoV (%)\": round(sov,2),\n",
    "            \"Positive SoV (%)\": round(pos_sov,2)\n",
    "        })\n",
    "    sov_df = pd.DataFrame(rows).sort_values(\"SoV (%)\", ascending=False).reset_index(drop=True)\n",
    "    return sov_df\n",
    "\n",
    "# ---------- Plot merged chart ----------\n",
    "def plot_sov_merged(sov_df, top_n=12):\n",
    "    df = sov_df.head(top_n)\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax.bar(df[\"brand\"], df[\"SoV (%)\"], label=\"SoV (%)\", alpha=0.7)\n",
    "    ax.set_ylabel(\"Share of Voice (%)\")\n",
    "    ax.set_xticklabels(df[\"brand\"], rotation=45, ha=\"right\")\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df[\"brand\"], df[\"Positive SoV (%)\"], color=\"red\", marker=\"o\", label=\"Positive SoV (%)\")\n",
    "    ax2.set_ylabel(\"Positive SoV (%)\")\n",
    "    ax.set_title(\"Combined Share of Voice (merged across keywords)\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------- Run pipeline ----------\n",
    "if __name__ == \"__main__\":\n",
    "    base_kw = f\"'{COMPETITORS}' in smart fans\"\n",
    "    keywords = generate_keywords_cohere(base_kw, max_k=MAX_KEYWORDS)\n",
    "    print(\"[INFO] Keywords used:\", keywords)\n",
    "\n",
    "    sov_df = calculate_sov(keywords, competitors=COMPETITORS, videos_per_kw=VIDEOS_PER_KEYWORD)\n",
    "    print(\"\\n=== Share of Voice Table ===\")\n",
    "    display(sov_df)  # in Colab this prints nicely\n",
    "    plot_sov_merged(sov_df)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
